{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2BUOcQ15hIS"
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Introduction\n",
    "- Cost Function\n",
    "- Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDDf5uu35hIU"
   },
   "source": [
    "## Linear Regression Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lH1rrn2X5hIY"
   },
   "source": [
    "Linear regression is a method of using supervised learning (where you have the input data along with the correct values) to determine a linear relationship between the independent (x) variables and the dependent (y) variable. Linear regression follows the basic simple steps: \n",
    "\n",
    "1. Plot the dependent variable against the independent variable\n",
    "2. Come up with a starting hypothesis function and measure its correlation with your data, similar to a line of best fit\n",
    "3. Find the error margins between the real data and your predicted points, and proceed to change the line's parameters to minimize the total error\n",
    "4. Repeat step 3 until it reaches the best possible correlation with the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNdwpmxlhPoK"
   },
   "source": [
    "At its core, linear regression follows a very simple idea. We start off with a hypothesis function and proceed to measure how much our hypothesis differs from the actual points. Using calculus and derivatives, we try to decrease this error until we finally reach a low point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can occur with one input variable or multiple input variables, but the methodology is very similar. For our purposes, we will focus on univariate linear regression, which uses 1 x value and 1 y value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZG0tc1Zg4of"
   },
   "source": [
    "<img src=\"images/generalGraph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfbuCJX4jYsX"
   },
   "source": [
    "NOTE: Linear regression is an algorithm applicable to many different types of machine learning programs. However, since it is an algorithm, this notebook will focus more on the mathematical approach than the coding aspect, though there is a short example project in the notebook LinearRegressionProject.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTbfClGChMxH"
   },
   "source": [
    "To use linear regression with machine learning, there are two main ideas: the cost function and gradient descent. The cost function is essentially a method to measure the accuracy of your hypothesis function by comparing its predicted values to the true data points. Gradient descent is a method to decrease the error margin. \n",
    "\n",
    "There are multiple alternative methods possible with linear regression, such as the normal equation. However, the cost function and gradient descent together are among the best to visualize and understand the workings behind Machine Learning, so we will focus on these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIt8HIhV5hIa"
   },
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwU77rWg5hIb"
   },
   "source": [
    "The cost function is a method to measure the accuracy of your hypothesis function. This essentially takes an average difference of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "Here is the cost function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqVtIxpB-vdv"
   },
   "source": [
    "![](https://drive.google.com/uc?export=download&id=1XafsTRf_j6o3ZN5_R1sLKt2950Dm-IXV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what you should actually focus on: \n",
    "\n",
    "<img src=\"images/focusingCostFunction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO_SCOLypIhe"
   },
   "source": [
    "### Dissecting the cost function\n",
    "\n",
    "<b>m:</b> the number of entries in our dataset\n",
    "\n",
    "<b>i:</b> which entry in the dataset we are on\n",
    "\n",
    "<b>h<sub>0</sub>(x<sub>i</sub>):</b> the current y value that our function predicted\n",
    "\n",
    "<b>y<sub>i</sub>:</b> the true y value from the dataset \n",
    "\n",
    "<b>REAL COST FUNCTION:</b> J(0, 1) = 1/2 * average of (error margin of predicted output vs real output)\n",
    "\n",
    "Within the parentheses, we see h<sub>0</sub>(x<sub>i</sub>), which represents the y value that our hypothesis function predicts, and y<sub>i</sub>, which represents the true y function. By subtracting these two, we find the exact value of how wrong our prediction was. We then square this value, which involves derivatives and allows us to find the absolute value of our error. Using segmas (the strange e), we are able to perform this operation for all coordinates in our dataset and add them together. We then multiply this value by 1/m, which finds the average error of our prediction. We then multiply by 1/2, which has been mathematically proven to help with computation during gradient descent (which we'll cover next). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJe-0Rqf5hIg"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9o5jvHg5hIi"
   },
   "source": [
    "At this point, we have a hypothesis function and can measure its fit to our given data. Our next goal is to decrease our error margin and hone in on parameters for our hypothesis function. We can do this using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEhFreSGjWob"
   },
   "source": [
    "We first graph our hypothesis function based on its fields of θ<sub>0</sub> and θ<sub>1</sub>. It is important to note that we are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters. We put θ<sub>0</sub> on the x axis and θ<sub>1</sub> on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sea-thMyk71J"
   },
   "source": [
    "![](https://drive.google.com/uc?export=download&id=1gK9su_V0ADoDiFvbtuTGM9QmP3Ujiw7y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gktvzky-in_b"
   },
   "source": [
    "Our end goal is to get our cost function the very bottom of the pits in our graph, which represent the minimum error possible. The red arrows show the minimum points in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kJFHYsZkTs6"
   },
   "source": [
    "We do this by taking the derivative of our cost function. You don't need to know calculus for this course, but for our purposes, the derivative tells you which direction you should move towards to decrease your error margin. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFLpb56NmNoP"
   },
   "source": [
    "With each run through, we test the hypothesis's error margin using the cost function. Using the derivative, we then determine how to decrease this error margin. We run through this process over and over again until we reach a low point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradientGraph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcBwNNGTktO3"
   },
   "source": [
    "One problem is that depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places. You select your starting point based on a case-by-case basis. If there are multiple local minimums, you can visualize it, check thousands of starting points, or use an alternative method such as the normal equation to determine a starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g75sZ73vIrBU"
   },
   "source": [
    "### Putting the cost function and gradient descent together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TYHsr1wgmjnK"
   },
   "source": [
    "We can put gradient descent and the cost function together through a new equation, listed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6Zufauim4ig"
   },
   "source": [
    "![](https://drive.google.com/uc?export=download&id=1SUEiwPX_Eh-AQdjl_ylv9Au_ypWiXZIj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dr8CzlEXm5O1"
   },
   "source": [
    "As you can see, the cost function makes up a big part of the new equation. With the section obtained from the cost function, we see that we have the average error of the hypothesis function compared to the true points. We then multiply that by the learning rate and subtract the new error margin from the old parameter, which gives you a new parameter with a lower error margin. By repeating this until convergence (reaching a low point), you are able to gradually increase the accuracy of your function. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LinearRegression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
